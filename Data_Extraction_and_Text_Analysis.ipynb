{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qa_ONewhIVLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509e28e3-2641-4224-e91e-78f5fa71d194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/.shortcut-targets-by-id/1ltdsXAS_zaZ3hI-q9eze_QCzHciyYAJY/20211030 Test Assignment\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd '/gdrive/MyDrive/project'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lNMH1gfF3cF",
        "outputId": "c0e24316-828d-4486-9953-18d9944c37f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the input Excel file\n",
        "df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Remove rows with specific URL_ID values\n",
        "# Specify the exact URL_ID values that correspond to 404 errors\n",
        "ids_to_drop = [44, 57, 144]\n",
        "df = df[~df['URL_ID'].isin(ids_to_drop)]\n",
        "\n",
        "# Directory to save the text files\n",
        "output_dir = '/gdrive/MyDrive/Project'  # Ensure this path is writable\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Function to extract the title and article text from a URL\n",
        "def extract_article_text(url):\n",
        "    header = {\n",
        "        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=header)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing content from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Extract title (adjust selectors based on actual HTML structure)\n",
        "    try:\n",
        "        title = soup.find('h1').get_text(strip=True)\n",
        "    except AttributeError:\n",
        "        print(f\"Cannot find title for {url}\")\n",
        "        title = \"No Title Found\"\n",
        "\n",
        "    # Extract article text (adjust selectors based on actual HTML structure)\n",
        "    article = \"\"\n",
        "    try:\n",
        "        # Assuming the article text is within <div> tags with a specific class or id\n",
        "        content_div = soup.find('div', class_='article-content') or soup.find('div', id='article-body') or soup.find('article')\n",
        "        if content_div:\n",
        "            for p in content_div.find_all('p'):\n",
        "                article += p.get_text(strip=True) + \"\\n\"\n",
        "        else:\n",
        "            print(f\"Cannot find article content for {url}\")\n",
        "    except AttributeError:\n",
        "        print(f\"Cannot find article text for {url}\")\n",
        "\n",
        "    return title, article\n",
        "\n",
        "# Loop through each URL and extract the text\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    if title and article_text:\n",
        "        file_path = os.path.join(output_dir, f'{url_id}.txt')\n",
        "        try:\n",
        "            with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(title + '\\n\\n' + article_text)\n",
        "            print(f'Successfully saved article {url_id}')\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving article {url_id}: {e}\")\n",
        "\n",
        "print(\"Data extraction completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ScdcfCgriUy",
        "outputId": "36687db4-5d46-416c-bdd2-08cab7bde732"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved article blackassign0001\n",
            "Successfully saved article blackassign0002\n",
            "Successfully saved article blackassign0003\n",
            "Successfully saved article blackassign0004\n",
            "Successfully saved article blackassign0005\n",
            "Successfully saved article blackassign0006\n",
            "Successfully saved article blackassign0007\n",
            "Successfully saved article blackassign0008\n",
            "Successfully saved article blackassign0009\n",
            "Successfully saved article blackassign0010\n",
            "Successfully saved article blackassign0011\n",
            "Successfully saved article blackassign0012\n",
            "Successfully saved article blackassign0013\n",
            "Successfully saved article blackassign0014\n",
            "Successfully saved article blackassign0015\n",
            "Successfully saved article blackassign0016\n",
            "Successfully saved article blackassign0017\n",
            "Successfully saved article blackassign0018\n",
            "Successfully saved article blackassign0019\n",
            "Successfully saved article blackassign0020\n",
            "Successfully saved article blackassign0021\n",
            "Successfully saved article blackassign0022\n",
            "Successfully saved article blackassign0023\n",
            "Successfully saved article blackassign0024\n",
            "Successfully saved article blackassign0025\n",
            "Successfully saved article blackassign0026\n",
            "Successfully saved article blackassign0027\n",
            "Successfully saved article blackassign0028\n",
            "Successfully saved article blackassign0029\n",
            "Successfully saved article blackassign0030\n",
            "Successfully saved article blackassign0031\n",
            "Successfully saved article blackassign0032\n",
            "Successfully saved article blackassign0033\n",
            "Successfully saved article blackassign0034\n",
            "Successfully saved article blackassign0035\n",
            "Error fetching https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Successfully saved article blackassign0037\n",
            "Successfully saved article blackassign0038\n",
            "Successfully saved article blackassign0039\n",
            "Successfully saved article blackassign0040\n",
            "Successfully saved article blackassign0041\n",
            "Successfully saved article blackassign0042\n",
            "Successfully saved article blackassign0043\n",
            "Successfully saved article blackassign0044\n",
            "Successfully saved article blackassign0045\n",
            "Successfully saved article blackassign0046\n",
            "Successfully saved article blackassign0047\n",
            "Successfully saved article blackassign0048\n",
            "Error fetching https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Successfully saved article blackassign0050\n",
            "Successfully saved article blackassign0051\n",
            "Successfully saved article blackassign0052\n",
            "Successfully saved article blackassign0053\n",
            "Successfully saved article blackassign0054\n",
            "Successfully saved article blackassign0055\n",
            "Successfully saved article blackassign0056\n",
            "Successfully saved article blackassign0057\n",
            "Successfully saved article blackassign0058\n",
            "Successfully saved article blackassign0059\n",
            "Successfully saved article blackassign0060\n",
            "Successfully saved article blackassign0061\n",
            "Successfully saved article blackassign0062\n",
            "Successfully saved article blackassign0063\n",
            "Successfully saved article blackassign0064\n",
            "Successfully saved article blackassign0065\n",
            "Successfully saved article blackassign0066\n",
            "Successfully saved article blackassign0067\n",
            "Successfully saved article blackassign0068\n",
            "Successfully saved article blackassign0069\n",
            "Successfully saved article blackassign0070\n",
            "Successfully saved article blackassign0071\n",
            "Successfully saved article blackassign0072\n",
            "Successfully saved article blackassign0073\n",
            "Successfully saved article blackassign0074\n",
            "Successfully saved article blackassign0075\n",
            "Successfully saved article blackassign0076\n",
            "Successfully saved article blackassign0077\n",
            "Successfully saved article blackassign0078\n",
            "Successfully saved article blackassign0079\n",
            "Successfully saved article blackassign0080\n",
            "Successfully saved article blackassign0081\n",
            "Successfully saved article blackassign0082\n",
            "Successfully saved article blackassign0083\n",
            "Successfully saved article blackassign0084\n",
            "Successfully saved article blackassign0085\n",
            "Successfully saved article blackassign0086\n",
            "Successfully saved article blackassign0087\n",
            "Successfully saved article blackassign0088\n",
            "Successfully saved article blackassign0089\n",
            "Successfully saved article blackassign0090\n",
            "Successfully saved article blackassign0091\n",
            "Successfully saved article blackassign0092\n",
            "Successfully saved article blackassign0093\n",
            "Successfully saved article blackassign0094\n",
            "Successfully saved article blackassign0095\n",
            "Successfully saved article blackassign0096\n",
            "Successfully saved article blackassign0097\n",
            "Successfully saved article blackassign0098\n",
            "Successfully saved article blackassign0099\n",
            "Successfully saved article blackassign0100\n",
            "Data extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Directories\n",
        "text_dir = \"/gdrive/MyDrive/Project\"\n",
        "stopwords_dir = \"/gdrive/MyDrive/project/StopWords\"\n",
        "sentment_dir = \"/gdrive/MyDrive/project/MasterDictionary\"\n",
        "\n",
        "# load all stop wors from the stopwords directory and store in the set variable\n",
        "stop_words = set()\n",
        "for files in os.listdir(stopwords_dir):\n",
        "  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n",
        "    stop_words.update(set(f.read().splitlines()))\n",
        "\n",
        "# load all text files  from the  directory and store in a list(docs)\n",
        "docs = []\n",
        "for text_file in os.listdir(text_dir):\n",
        "  with open(os.path.join(text_dir,text_file),'r') as f:\n",
        "    text = f.read()\n",
        "#tokenize the given text file\n",
        "    words = word_tokenize(text)\n",
        "# remove the stop words from the tokens\n",
        "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "# add each filtered tokens of each file into a list\n",
        "    docs.append(filtered_text)\n",
        "\n",
        "\n",
        "\n",
        "# store positive, Negative words from the directory\n",
        "pos=set()\n",
        "neg=set()\n",
        "\n",
        "for files in os.listdir(sentment_dir):\n",
        "  if files =='positive-words.txt':\n",
        "    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n",
        "      pos.update(f.read().splitlines())\n",
        "  else:\n",
        "    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n",
        "      neg.update(f.read().splitlines())\n",
        "\n",
        "# now collect the positive  and negative words from each file\n",
        "# calculate the scores from the positive and negative words\n",
        "positive_words = []\n",
        "Negative_words =[]\n",
        "positive_score = []\n",
        "negative_score = []\n",
        "polarity_score = []\n",
        "subjectivity_score = []\n",
        "\n",
        "#iterate through the list of docs\n",
        "for i in range(len(docs)):\n",
        "  positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
        "  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
        "  positive_score.append(len(positive_words[i]))\n",
        "  negative_score.append(len(Negative_words[i]))\n",
        "  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
        "  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n"
      ],
      "metadata": {
        "id": "1tRdSv8ErMOm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "# Average Sentence Length = the number of words / the number of sentences\n",
        "# Percentage of Complex words = the number of complex words / the number of words\n",
        "# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
        "\n",
        "avg_sentence_length = []\n",
        "Percentage_of_Complex_words  =  []\n",
        "Fog_Index = []\n",
        "complex_word_count =  []\n",
        "avg_syllable_word_count =[]\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "def measure(file):\n",
        "  with open(os.path.join(text_dir, file),'r') as f:\n",
        "    text = f.read()\n",
        "# remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s.]','',text)\n",
        "# split the given text file into sentences\n",
        "    sentences = text.split('.')\n",
        "# total number of sentences in a file\n",
        "    num_sentences = len(sentences)\n",
        "# total words in the file\n",
        "    words = [word  for word in text.split() if word.lower() not in stopwords ]\n",
        "    num_words = len(words)\n",
        "\n",
        "# complex words having syllable count is greater than 2\n",
        "# Complex words are words in the text that contain more than two syllables.\n",
        "    complex_words = []\n",
        "    for word in words:\n",
        "      vowels = 'aeiou'\n",
        "      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
        "      if syllable_count_word > 2:\n",
        "        complex_words.append(word)\n",
        "\n",
        "# Syllable Count Per Word\n",
        "# We count the number of Syllables in each word of the text by counting the vowels present in each word.\n",
        "#  We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
        "    syllable_count = 0\n",
        "    syllable_words =[]\n",
        "    for word in words:\n",
        "      if word.endswith('es'):\n",
        "        word = word[:-2]\n",
        "      elif word.endswith('ed'):\n",
        "        word = word[:-2]\n",
        "      vowels = 'aeiou'\n",
        "      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
        "      if syllable_count_word >= 1:\n",
        "        syllable_words.append(word)\n",
        "        syllable_count += syllable_count_word\n",
        "\n",
        "\n",
        "    avg_sentence_len = num_words / num_sentences\n",
        "    avg_syllable_word_count = syllable_count / len(syllable_words)\n",
        "    Percent_Complex_words  =  len(complex_words) / num_words\n",
        "    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
        "\n",
        "    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\n",
        "\n",
        "# iterate through each file or doc\n",
        "for file in os.listdir(text_dir):\n",
        "  x,y,z,a,b = measure(file)\n",
        "  avg_sentence_length.append(x)\n",
        "  Percentage_of_Complex_words.append(y)\n",
        "  Fog_Index.append(z)\n",
        "  complex_word_count.append(a)\n",
        "  avg_syllable_word_count.append(b)"
      ],
      "metadata": {
        "id": "F8RaMuD_EnQQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n",
        "# We count the total cleaned words present in the text by\n",
        "# removing the stop words (using stopwords class of nltk package).\n",
        "# removing any punctuations like ? ! , . from the word before counting.\n",
        "\n",
        "def cleaned_words(file):\n",
        "  with open(os.path.join(text_dir,file), 'r') as f:\n",
        "    text = f.read()\n",
        "    text = re.sub(r'[^\\w\\s]', '' , text)\n",
        "    words = [word  for word in text.split() if word.lower() not in stopwords]\n",
        "    length = sum(len(word) for word in words)\n",
        "    average_word_length = length / len(words)\n",
        "  return len(words),average_word_length\n",
        "\n",
        "word_count = []\n",
        "average_word_length = []\n",
        "for file in os.listdir(text_dir):\n",
        "  x, y = cleaned_words(file)\n",
        "  word_count.append(x)\n",
        "  average_word_length.append(y)\n",
        "\n",
        "\n",
        "# To calculate Personal Pronouns mentioned in the text, we use regex to find\n",
        "# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n",
        "#  so that the country name US is not included in the list.\n",
        "def count_personal_pronouns(file):\n",
        "  with open(os.path.join(text_dir,file), 'r') as f:\n",
        "    text = f.read()\n",
        "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "    count = 0\n",
        "    for pronoun in personal_pronouns:\n",
        "      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
        "  return count\n",
        "\n",
        "pp_count = []\n",
        "for file in os.listdir(text_dir):\n",
        "  x = count_personal_pronouns(file)\n",
        "  pp_count.append(x)"
      ],
      "metadata": {
        "id": "4NElx7d94ICm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
        "\n",
        "# URL_ID 44 ,57, 144 does not exists i,e. page does not exist, throughs 404 error\n",
        "# so we are going to drop these rows from the table\n",
        "output_df.drop([36,49], axis = 0, inplace=True)\n",
        "\n",
        "# These are the required parameters\n",
        "variables = [positive_score,\n",
        "            negative_score,\n",
        "            polarity_score,\n",
        "            subjectivity_score,\n",
        "            avg_sentence_length,\n",
        "            Percentage_of_Complex_words,\n",
        "            Fog_Index,\n",
        "            avg_sentence_length,\n",
        "            complex_word_count,\n",
        "            word_count,\n",
        "            avg_syllable_word_count,\n",
        "            pp_count,\n",
        "            average_word_length]\n",
        "\n",
        "# write the values to the dataframe\n",
        "for i, var in enumerate(variables):\n",
        "  output_df.iloc[:,i+2] = var\n"
      ],
      "metadata": {
        "id": "mXsnVluZ9TG3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now save the dataframe to the disk\n",
        "%cd '/gdrive/MyDrive/Project'\n",
        "output_df.to_csv('Output_Data1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pABWzozqN6zy",
        "outputId": "cc6999bd-2abd-49d4-d6a3-78de970d5daa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Project\n"
          ]
        }
      ]
    }
  ]
}